.. Copyright 2019-2020 University of Maryland and other Hatchet Project
   Developers. See the top-level LICENSE file for details.

   SPDX-License-Identifier: MIT

Analysis Examples
=================

Applying scalar operations to attributes
----------------------------------------

Individual numeric columns in the dataframe can be scaled or offset by a
constant using the native pandas operations. We make a copy of the original
graphframe, and modify the dataframe directly. In this example, we offset the
``time`` column by -2 and scale it by 1/1e7, storing the result in a new column
in the dataframe called ``scaled time``.

.. code-block:: python

  gf = ht.GraphFrame.from_hpctoolkit('kripke')
  gf.drop_index_levels()

  offset = 1e7
  gf.dataframe['scaled time'] = (gf.dataframe['time'] / offset) - 2
  sorted_df = gf.dataframe.sort_values(by=['scaled time'], ascending=False)
  print(sorted_df)

.. image:: images/scale-offset-df.png
   :scale: 40 %


Generating a flat profile
-------------------------

It is straightforward to generate a flat profile in hatchet by using the
``groupby`` functionality in pandas. The flat profile can be based on any
categorical column (e.g., function name, load module, file name).  We can
transform the tree or graph generated by a profiler into a flat profile by
specifying the column on which to apply the ``groupby`` operation and the
function to use for aggregation.  In the example below, we apply a pandas
``groupby`` operation on the ``name`` column.  Using a ``sum`` to aggregate
rows in a group gives us the time spent in each function. We then display the
resulting dataframe sorted by time.

.. image:: images/flat-function.png
   :scale: 30 %
   :align: right

.. code-block:: python

  gf = ht.GraphFrame.from_hpctoolkit('kripke')
  gf.drop_index_levels()

  grouped = gf.dataframe.groupby('name').sum()
  sorted_df = grouped.sort_values(by=['time'],
                                  ascending=False)
  print(sorted_df)


Identifying load imbalance
--------------------------

Hatchet makes it extremely easy to study load imbalance across processes or
threads at the per-node granularity (call site or function level).  A typical
metric to measure imbalance is to look at the ratio of the maximum and average
time spent in a code region across all processes.

We can identify the load imbalance at each node by perform a
``drop_index_levels`` operation on the GraphFrame in two different ways: by
providing mean as a function in one case and max as the function to another
copy of the DataFrame. This generates two DataFrames, one containing the
average time spent in each node, and the other containing the maximum time
spent in each node by any process. If we divide the corresponding columns of
the two DataFrames and look at the nodes with the highest value of the
max-to-average ratio, we have located the nodes with highest imbalance.

.. code-block:: python

  gf1 = ht.GraphFrame.from_caliper('lulesh-512cores')
  gf2 = gf1.copy()

  gf1.drop_index_levels(function=np.mean)
  gf2.drop_index_levels(function=np.max)

  gf1.dataframe['imbalance'] = gf2.dataframe['time'].div(gf1.dataframe['time'])

  sorted_df = gf1.dataframe.sort_values(by=['imbalance'], ascending=False)
  print(sorted_df)

.. image:: images/lulesh-load-imb.png
   :scale: 30 %


Comparing multiple executions
-----------------------------

An important task in parallel performance analysis is comparing the performance
of an application on two different thread counts or process counts.  The
filter, squash and subtract operations provided by the Hatchet API can be
extremely powerful in comparing profiling datasets from two executions.

In the example below, we ran LULESH on two core counts: 1 core and 27 cores,
and wanted to identify the performance changes as one scales on a node. We
subtract the graphframe at 27 cores from the graphframe at 1 core (after
dropping the additional index level), and sort the resulting graphframe by
execution time.

.. code-block:: python

  gf1 = ht.GraphFrame.from_caliper('lulesh-1core.json')
  gf2 = ht.GraphFrame.from_caliper('lulesh-27cores.json')

  gf2.drop_index_levels()
  gf3 = gf2 - gf1

  sorted_df = gf3.dataframe.sort_values(by=['time'], ascending=False)
  print(sorted_df)

.. image:: images/lulesh-diff-df.png
   :scale: 40 %

Filtering by library
--------------------

Sometimes, users are interested in analyzing how a particular library such as
PetSc or MPI is used by their application and how the time spent in the library
changes as we scale to larger number of processes.

In this next example, we compare two datasets generated from executions at
different number of MPI processes. We read in two datasets of LULESH at 27 and
512 MPI processes respectively, and filter them both on the name column by
matching the names against ^MPI. After the filtering operation, we squash the
DataFrames to generate GraphFrames that just contain the MPI calls from the
original datasets. We can now subtract the squashed datasets to identify the
biggest offenders.

.. code-block:: python

  gf1 = GraphFrame.from_caliper('lulesh-27cores')
  gf1.drop_index_levels()
  filtered_gf1 = gf1.filter(lambda x: x['name'].startswith('MPI'))
  squashed_gf1 = filtered_gf1.squash()

  gf2 = GraphFrame.from_caliper('lulesh-512cores')
  gf2.drop_index_levels()
  filtered_gf2 = gf2.filter(lambda x: x['name'].startswith('MPI'))
  squashed_gf2 = filtered_gf2.squash()

  diff_gf = squashed_gf2 - squashed_gf1

  sorted_df = diff_gf.dataframe.sort_values(by=['time'], ascending=False)
  print(sorted_df)

.. image:: images/lulesh-mpi.png
   :scale: 40 %
